{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING IN PRODUCTION MADRID - MLFLOW DEPLOYMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons we've seen how to put a simple Scikit-Learn model into production. However, in the real world the models used to be complicated, maybe not Sklearn flavor and there is an important feature engineering of the input data.\n",
    "\n",
    "You can also handle that with MLFlow. We'll see how to do it in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model to Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is defining the paths to the pickle data we saved in previous lessons, in order to be able to reproduce the prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T08:34:43.140071Z",
     "start_time": "2020-02-16T08:34:43.137129Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle_data_path = '../output/pickle_data'\n",
    "\n",
    "artifacts = {\n",
    "    'encoder_path': f'{pickle_data_path}/encoder.pickle',\n",
    "    'umap_path': f'{pickle_data_path}/umap.pickle',\n",
    "    'hdbscan_path': f'{pickle_data_path}/hdbscan.pickle',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put a model into production with MLFlow it is necessary to define a wrapper for it. The process is straightforward with a Scikit-Learn model (KMeans from previous lessons) since the Sklearn Wrapper has been already defined by MLFlow developers.\n",
    "\n",
    "Thus, the only thing we need to do is extend the mlflow.pyfunc.PythonModel class and override the predict method:\n",
    "\n",
    "```python\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        your_code_here\n",
    "    \n",
    "```\n",
    "\n",
    "In the cell below, a custom mlflow.pyfunc.PythonModel has been defined. However, it is more complex than the previous definition since the feature engineering of the input data is also included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T08:34:47.639264Z",
     "start_time": "2020-02-16T08:34:46.144739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/centos/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/centos/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import hdbscan\n",
    "\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    # define some useful list of columns\n",
    "    def __init__(self):\n",
    "\n",
    "        self.columns_to_encode = ['origin', 'destination', 'train_type', 'train_class', 'fare']\n",
    "        self.columns_to_remove = ['insert_date', 'start_date', 'end_date']\n",
    "\n",
    "    # at the time of loading the MLFlow model, the pickle data from the baseline\n",
    "    # pipeline has to be loaded\n",
    "    def load_context(self, context):\n",
    "        \n",
    "        with open(context.artifacts['encoder_path'], 'rb') as f:\n",
    "            self.encoder_m = pickle.load(f)\n",
    "            \n",
    "        with open(context.artifacts['umap_path'], 'rb') as f:\n",
    "            self.umap_m = pickle.load(f)\n",
    "        \n",
    "        with open(context.artifacts['hdbscan_path'], 'rb') as f:\n",
    "            self.hdbscan_m = pickle.load(f)\n",
    "            \n",
    "    # the datetime columns could arrive in the integer form, in that case convert to\n",
    "    # datetime type\n",
    "    def check_dt_type(self, model_input):\n",
    "        \n",
    "        if model_input[self.columns_to_remove[0]].dtype == 'int64':\n",
    "            for col in self.columns_to_remove:\n",
    "                model_input[col] = pd.to_datetime(model_input[col])\n",
    "        \n",
    "        return model_input\n",
    "\n",
    "    # the baseline transformations are done here\n",
    "    def transform(self, model_input):\n",
    "        \n",
    "        model_input.dropna(inplace=True)\n",
    "        \n",
    "        model_input = self.check_dt_type(model_input)\n",
    "        \n",
    "        model_input.loc[:, self.columns_to_encode] = \\\n",
    "            self.encoder_m.transform(model_input[self.columns_to_encode])\n",
    "        \n",
    "        model_input['duration'] = (model_input['end_date'] - model_input['start_date']).dt.seconds / 3600\n",
    "\n",
    "        model_input['time_to_departure'] = (model_input['start_date'].dt.tz_localize('Europe/Madrid').dt.tz_convert('UTC') \\\n",
    "                                   - model_input['insert_date'].dt.tz_localize('UTC')).dt.days\n",
    "\n",
    "        model_input['hour'] = model_input['start_date'].dt.hour\n",
    "\n",
    "        model_input['weekday'] = model_input['start_date'].dt.dayofweek\n",
    "\n",
    "        model_input = model_input[[x for x in model_input.columns if x not in self.columns_to_remove]]\n",
    "        \n",
    "        return model_input\n",
    "\n",
    "    # main method to override, the OrdinalEncoder and UMAP transformations are done along\n",
    "    # with the HDBSCAN prediction over this embedding\n",
    "    def predict(self, context, model_input):\n",
    "        \n",
    "        # allocate payload with return value for null\n",
    "        payload = np.ones(len(model_input)) * -1\n",
    "        \n",
    "        preprocessed = self.transform(model_input.reset_index(drop=True))\n",
    "        embedding = self.umap_m.transform(preprocessed)\n",
    "        clusters, _ = hdbscan.approximate_predict(self.hdbscan_m, embedding)\n",
    "        \n",
    "        # fill not null records with their cluster\n",
    "        payload[preprocessed.index] = clusters\n",
    "        \n",
    "        return payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the custom model has been defined, it is necessary to pack everything together, both the model and the conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T08:34:57.803269Z",
     "start_time": "2020-02-16T08:34:57.610722Z"
    }
   },
   "outputs": [],
   "source": [
    "mlflow_pyfunc_model_path = '../output/custom_model'\n",
    "\n",
    "# remove all models if already there\n",
    "!rm -rf $mlflow_pyfunc_model_path\n",
    "\n",
    "# conda environment definition\n",
    "# As corporate proxy restricts us from reaching internet hence removing 'defaults'\n",
    "conda_env = {\n",
    "    'channels': [\n",
    "          '- https://nexus.apps.usae-2.syngentaaws.org/repository/Anaconda/pkgs/main/',\n",
    "          '- https://nexus.apps.usae-2.syngentaaws.org/repository/Anaconda/pkgs/free/',\n",
    "          '- https://nexus.apps.usae-2.syngentaaws.org/repository/Anaconda/pkgs/r/',\n",
    "          '- https://nexus.apps.usae-2.syngentaaws.org/repository/Anaconda/pkgs/pro/',\n",
    "          '- https://nexus.apps.usae-2.syngentaaws.org/repository/Anaconda/pkgs/msys2/'],\n",
    "    'dependencies': [\n",
    "        'python',\n",
    "        {'pip': [\n",
    "            'mlflow',\n",
    "            'umap-learn',\n",
    "            'hdbscan',\n",
    "          ]\n",
    "        },\n",
    "    ],\n",
    "    'name': 'custom_env',\n",
    "}\n",
    "\n",
    "# finally save the model as an MLFlow project into the output directory\n",
    "mlflow.pyfunc.save_model(path=mlflow_pyfunc_model_path, \n",
    "                         python_model=ModelWrapper(),\n",
    "                         conda_env=conda_env,\n",
    "                         artifacts=artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons we saw how to create an endpoint with MLFlow and the command line:\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m path_to_your_model -h host -p port\n",
    "```\n",
    "\n",
    "However, it is desirable that this endpoint could be always alive. This can be done with systemd and the following configuration:\n",
    "\n",
    "```\n",
    "[Unit]\n",
    "Description=MLFlow model in production\n",
    "After=network.target\n",
    "\n",
    "[Service]\n",
    "Restart=on-failure\n",
    "RestartSec=30\n",
    "StandardOutput=file:/path_to_your_logging_folder/stdout.log\n",
    "StandardError=file:/path_to_your_logging_folder/stderr.log\n",
    "Environment=MLFLOW_TRACKING_URI=http://host_ts:port_ts\n",
    "Environment=MLFLOW_CONDA_HOME=/path_to_your_conda_installation\n",
    "ExecStart=/bin/bash -c 'PATH=/path_to_your_conda_installation/envs/mlinproduction_env/bin/:$PATH exec mlflow models serve -m path_to_your_model -h host -p port'\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing the endpoint it is necessary to load some test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T08:35:08.794384Z",
     "start_time": "2020-02-16T08:35:06.676243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insert_date</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>train_type</th>\n",
       "      <th>price</th>\n",
       "      <th>train_class</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2654730</td>\n",
       "      <td>2019-09-17 09:15:29</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-09-23 07:40:00</td>\n",
       "      <td>2019-09-23 10:05:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>79.65</td>\n",
       "      <td>Preferente</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9343390</td>\n",
       "      <td>2019-06-18 01:00:55</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>2019-08-15 13:10:00</td>\n",
       "      <td>2019-08-15 20:51:00</td>\n",
       "      <td>MD-LD</td>\n",
       "      <td>34.35</td>\n",
       "      <td>Turista con enlace</td>\n",
       "      <td>Promo +</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784700</td>\n",
       "      <td>2019-08-29 09:45:54</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>2019-09-21 08:30:00</td>\n",
       "      <td>2019-09-21 11:15:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>85.10</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7666853</td>\n",
       "      <td>2019-05-30 06:00:56</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-06-22 08:45:00</td>\n",
       "      <td>2019-06-22 20:16:00</td>\n",
       "      <td>MD</td>\n",
       "      <td>52.50</td>\n",
       "      <td>Turista con enlace</td>\n",
       "      <td>Flexible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4388336</td>\n",
       "      <td>2019-04-20 09:05:17</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>2019-05-30 19:30:00</td>\n",
       "      <td>2019-05-30 22:40:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>75.40</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5556875</td>\n",
       "      <td>2019-05-05 09:26:47</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>2019-05-12 16:00:00</td>\n",
       "      <td>2019-05-12 18:30:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>76.30</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Flexible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721056</td>\n",
       "      <td>2019-08-28 19:05:33</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-10-06 20:00:00</td>\n",
       "      <td>2019-10-06 23:10:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>80.15</td>\n",
       "      <td>Turista Plus</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3366351</td>\n",
       "      <td>2019-04-12 23:39:28</td>\n",
       "      <td>SEVILLA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-05-25 19:45:00</td>\n",
       "      <td>2019-05-25 22:17:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6943674</td>\n",
       "      <td>2019-05-22 17:49:39</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-07-15 12:50:00</td>\n",
       "      <td>2019-07-15 15:45:00</td>\n",
       "      <td>AVE-TGV</td>\n",
       "      <td>75.40</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3739460</td>\n",
       "      <td>2019-04-15 16:12:31</td>\n",
       "      <td>VALENCIA</td>\n",
       "      <td>MADRID</td>\n",
       "      <td>2019-04-30 18:10:00</td>\n",
       "      <td>2019-04-30 19:57:00</td>\n",
       "      <td>AVE</td>\n",
       "      <td>33.65</td>\n",
       "      <td>Turista</td>\n",
       "      <td>Promo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                insert_date     origin destination          start_date  \\\n",
       "2654730 2019-09-17 09:15:29    SEVILLA      MADRID 2019-09-23 07:40:00   \n",
       "9343390 2019-06-18 01:00:55     MADRID     SEVILLA 2019-08-15 13:10:00   \n",
       "784700  2019-08-29 09:45:54     MADRID   BARCELONA 2019-09-21 08:30:00   \n",
       "7666853 2019-05-30 06:00:56    SEVILLA      MADRID 2019-06-22 08:45:00   \n",
       "4388336 2019-04-20 09:05:17     MADRID   BARCELONA 2019-05-30 19:30:00   \n",
       "5556875 2019-05-05 09:26:47     MADRID     SEVILLA 2019-05-12 16:00:00   \n",
       "721056  2019-08-28 19:05:33  BARCELONA      MADRID 2019-10-06 20:00:00   \n",
       "3366351 2019-04-12 23:39:28    SEVILLA      MADRID 2019-05-25 19:45:00   \n",
       "6943674 2019-05-22 17:49:39  BARCELONA      MADRID 2019-07-15 12:50:00   \n",
       "3739460 2019-04-15 16:12:31   VALENCIA      MADRID 2019-04-30 18:10:00   \n",
       "\n",
       "                   end_date train_type  price         train_class      fare  \n",
       "2654730 2019-09-23 10:05:00        AVE  79.65          Preferente     Promo  \n",
       "9343390 2019-08-15 20:51:00      MD-LD  34.35  Turista con enlace   Promo +  \n",
       "784700  2019-09-21 11:15:00        AVE  85.10             Turista     Promo  \n",
       "7666853 2019-06-22 20:16:00         MD  52.50  Turista con enlace  Flexible  \n",
       "4388336 2019-05-30 22:40:00        AVE  75.40             Turista     Promo  \n",
       "5556875 2019-05-12 18:30:00        AVE  76.30             Turista  Flexible  \n",
       "721056  2019-10-06 23:10:00        AVE  80.15        Turista Plus     Promo  \n",
       "3366351 2019-05-25 22:17:00        AVE    NaN             Turista     Promo  \n",
       "6943674 2019-07-15 15:45:00    AVE-TGV  75.40             Turista     Promo  \n",
       "3739460 2019-04-30 19:57:00        AVE  33.65             Turista     Promo  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../data/raw/renfe.parquet')\n",
    "\n",
    "test_data = df.sample(10)\n",
    "\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the endpoint is not working as expected, the model can be loaded with the MLFlow API into the Jupyter notebook and start debugging it with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T08:35:41.797669Z",
     "start_time": "2020-02-16T08:35:35.122153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [ 9.  3.  5.  3.  9.  8.  9. -1.  9.  3.]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n",
    "\n",
    "print(f'Predictions: {loaded_model.predict(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is done via Python requests, however it can also be done with cURL or another tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T11:44:11.968385Z",
     "start_time": "2020-03-24T11:44:11.798459Z"
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66e8665fc406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Predictions: {r.text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'usaeilidssbxd01.syngentaaws.org'\n",
    "port = '8900'\n",
    "\n",
    "url = f'http://{host}:{port}/invocations'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "r = requests.post(url=url, headers=headers, data=test_data.to_json(orient='split'))\n",
    "\n",
    "print(f'Predictions: {r.text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "mlflow-sagemaker",
   "language": "python",
   "name": "mlflow-sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
