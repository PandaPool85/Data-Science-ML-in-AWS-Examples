{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training Job \n",
    "\n",
    "### Please go through this notebook only if you have finished Part 1 to Part 4 of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Import packages, get IAM role, get the region and set the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T12:40:59.982838Z",
     "start_time": "2020-04-16T12:40:30.475824Z"
    }
   },
   "outputs": [],
   "source": [
    "%%sh \n",
    "pwd\n",
    "cd local_test/test_dir/input/data/training/\n",
    "ls -ltr\n",
    "aws s3 cp data_set link to s3 bucket containing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Create the algorithm image and push to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T11:31:57.935924Z",
     "start_time": "2020-04-16T11:31:57.922674Z"
    }
   },
   "outputs": [],
   "source": [
    "#Do not execute this\n",
    "%%sh\n",
    "\n",
    "chmod +x create_container.sh \n",
    "\n",
    "./create_container.sh keras-sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Define variables with data location and output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:14.184980Z",
     "start_time": "2020-04-17T07:38:14.180644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://link to dataset\n",
      "output location - s3://link to output\n"
     ]
    }
   ],
   "source": [
    "schema = 's3:/' \n",
    "bucket = 'eu.com.syngenta-datascience-model-training'\n",
    "\n",
    "user = 'username' \n",
    "experiment = 'keras-sagemaker'\n",
    "\n",
    "\n",
    "data_location = f'{schema}/{bucket}/{user}/{ experiment}/train/data'\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = f'{schema}/{bucket}/{user}/{ experiment}/output'\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Create a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:19.452546Z",
     "start_time": "2020-04-17T07:38:18.541862Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "# this line of code requires iam:GetRole permissions\n",
    "#role = sage.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Define variables for account, region and algorithm image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:30.717793Z",
     "start_time": "2020-04-17T07:38:30.678975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170605107178\n",
      "eu-central-1\n",
      "170605107178.dkr.ecr.eu-central-1.amazonaws.com/datascience-model-training:cpu\n"
     ]
    }
   ],
   "source": [
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account'] # aws account \n",
    "region = sagemaker_session.boto_session.region_name # aws server region\n",
    "tag='cpu'\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/datascience-model-training:{}'.format(account, region, tag) # algorithm image path in ECR\n",
    "#keras-sagemaker-train\n",
    "print(account)\n",
    "print(region)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Define hyperparameters to be passed to your algorithm. \n",
    "In this project we are reading two hyperparameters for training. Use of hyperparameters in optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:35.177212Z",
     "start_time": "2020-04-17T07:38:35.174523Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\":128, \"epochs\":30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 7: Create the training job using SageMaker Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:39.892970Z",
     "start_time": "2020-04-17T07:38:39.888424Z"
    }
   },
   "outputs": [],
   "source": [
    "role = 'arn:aws:iam::170605107178:role/SYN-Datascience-SageMaker-Role'\n",
    "\n",
    "subnets_config = ['subnet-0bdd33f41f946b22a', 'subnet-0c7c8959343746db7']\n",
    "\n",
    "security_groups_config = [ \"sg-1ad4ea70\",\n",
    "              \"sg-99d781f1\",\n",
    "              \"sg-dfa1f7b7\"]\n",
    "\n",
    "# the instance type to be used for training. using 'local' will not trigger a job on SageMaker\n",
    "instance_count = 1\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "\n",
    "#https://aws.amazon.com/sagemaker/pricing/\n",
    "#https://aws.amazon.com/sagemaker/pricing/instance-types/\n",
    "\n",
    "#ml.p3.2xlarge Accelerated Computing â€“ Current Generation \t8(CPU cores)\t1xV100(GPU)\t61GiB(CPU mem)\t16GiB(Gpu mem)\n",
    "# ml.c5.2xlarge \t  Compute Instances - Current Generation\t $0.543 16GiB RAM 8 cores\n",
    "# ml.m4.xlarge\t   Compute Instances - Standard Generation\t $0.336 16GiB Ram 4 cores\n",
    "\n",
    "\n",
    "classifier = sage.estimator.Estimator(image_name=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=instance_count, \n",
    "                                      train_instance_type= instance_type,\n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      output_path=output_location,\n",
    "                                      subnets=subnets_config, \n",
    "                                      security_group_ids=security_groups_config,\n",
    "                                      sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 8: Run the training job by passing the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:38:43.782434Z",
     "start_time": "2020-04-17T07:38:43.778790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datascience-model-training-2020-04-17-07-38-43\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "job_prefix_name = 'datascience-model-training'\n",
    "training_job_name = job_prefix_name + '-'  + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:43:13.954953Z",
     "start_time": "2020-04-17T07:38:46.613349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-17 07:38:47 Starting - Starting the training job...\n",
      "2020-04-17 07:38:49 Starting - Launching requested ML instances......\n",
      "2020-04-17 07:40:13 Starting - Preparing the instances for training......\n",
      "2020-04-17 07:41:21 Downloading - Downloading input data\n",
      "2020-04-17 07:41:21 Training - Downloading the training image......\n",
      "2020-04-17 07:42:16 Training - Training image download completed. Training in progress..\u001b[34m2020-04-17 07:42:19.578385: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[34m2020-04-17 07:42:19.637425: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300065000 Hz\u001b[0m\n",
      "\u001b[34m2020-04-17 07:42:19.637901: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56247946f1e0 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[34m2020-04-17 07:42:19.637933: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[34m2020-04-17 07:42:27.374691: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[0m\n",
      "\u001b[34m[name: \"/device:CPU:0\"\u001b[0m\n",
      "\u001b[34mdevice_type: \"CPU\"\u001b[0m\n",
      "\u001b[34mmemory_limit: 268435456\u001b[0m\n",
      "\u001b[34mlocality {\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mincarnation: 2422872793627065326\u001b[0m\n",
      "\u001b[34m, name: \"/device:XLA_CPU:0\"\u001b[0m\n",
      "\u001b[34mdevice_type: \"XLA_CPU\"\u001b[0m\n",
      "\u001b[34mmemory_limit: 17179869184\u001b[0m\n",
      "\u001b[34mlocality {\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mincarnation: 17409246217133759230\u001b[0m\n",
      "\u001b[34mphysical_device_desc: \"device: XLA_CPU device\"\u001b[0m\n",
      "\u001b[34m]\n",
      "\u001b[0m\n",
      "\u001b[34mScript Status - Starting\n",
      "\u001b[0m\n",
      "\u001b[34mReading hyper parameters\u001b[0m\n",
      "\u001b[34mFinished reading the hyper parameters.\n",
      "\u001b[0m\n",
      "\u001b[34mReading the data.\n",
      "\u001b[0m\n",
      "\u001b[34mNumber of data samples:  10000\u001b[0m\n",
      "\u001b[34mNumber of data labels:  10000\n",
      "\u001b[0m\n",
      "\u001b[34mNumber of training samples:  ((8000, 784), (8000, 10))\u001b[0m\n",
      "\u001b[34mNumber of test samples:  ((2000, 784), (2000, 10))\u001b[0m\n",
      "\u001b[34mFinished data reading and pre-processing.\n",
      "\u001b[0m\n",
      "\u001b[34mStarting the model training\u001b[0m\n",
      "\u001b[34mModel: \"sequential_1\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mdense_1 (Dense)              (None, 512)               401920    \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout_1 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_2 (Dense)              (None, 512)               262656    \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout_2 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_3 (Dense)              (None, 10)                5130      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 669,706\u001b[0m\n",
      "\u001b[34mTrainable params: 669,706\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 10s - loss: 2.3024 - accuracy: 0.1406\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 1s - loss: 2.2916 - accuracy: 0.1797 \u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 2.2711 - accuracy: 0.2484\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 2.2318 - accuracy: 0.3200\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 2.1880 - accuracy: 0.3453\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 2.1429 - accuracy: 0.3617\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 2.0828 - accuracy: 0.3878\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 2.0257 - accuracy: 0.4073\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 1.9658 - accuracy: 0.4230\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 1s 87us/step - loss: 1.9293 - accuracy: 0.4329 - val_loss: 1.3342 - val_accuracy: 0.6600\u001b[0m\n",
      "\u001b[34mEpoch 2/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.3960 - accuracy: 0.6016\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 1.3366 - accuracy: 0.6055\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 1.3080 - accuracy: 0.6052\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 1.2775 - accuracy: 0.6065\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 1.2364 - accuracy: 0.6245\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 1.2010 - accuracy: 0.6343\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 1.1695 - accuracy: 0.6430\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 1.1405 - accuracy: 0.6516\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 1.1087 - accuracy: 0.6637\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 1.0966 - accuracy: 0.6670 - val_loss: 0.7865 - val_accuracy: 0.7955\u001b[0m\n",
      "\u001b[34mEpoch 3/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.8698 - accuracy: 0.7344\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.9012 - accuracy: 0.7012\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.8783 - accuracy: 0.7193\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.8660 - accuracy: 0.7216\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.8399 - accuracy: 0.7365\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.8248 - accuracy: 0.7388\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.8152 - accuracy: 0.7424\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.8033 - accuracy: 0.7470\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.7884 - accuracy: 0.7536\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.7863 - accuracy: 0.7556 - val_loss: 0.5795 - val_accuracy: 0.8590\u001b[0m\n",
      "\u001b[34mEpoch 4/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.7051 - accuracy: 0.7812\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.6883 - accuracy: 0.7959\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.6763 - accuracy: 0.8078\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.6548 - accuracy: 0.8072\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.6467 - accuracy: 0.8093\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.6466 - accuracy: 0.8090\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.6349 - accuracy: 0.8118\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.6376 - accuracy: 0.8091\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.6302 - accuracy: 0.8120\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.6299 - accuracy: 0.8119 - val_loss: 0.4702 - val_accuracy: 0.8735\u001b[0m\n",
      "\u001b[34mEpoch 5/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.6061 - accuracy: 0.8125\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.6031 - accuracy: 0.8212\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.5734 - accuracy: 0.8240\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 0s - loss: 0.5605 - accuracy: 0.8325\u001b[0m\n",
      "\u001b[34m4096/8000 [==============>...............] - ETA: 0s - loss: 0.5541 - accuracy: 0.8342\u001b[0m\n",
      "\u001b[34m5120/8000 [==================>...........] - ETA: 0s - loss: 0.5597 - accuracy: 0.8311\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.5514 - accuracy: 0.8324\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.5427 - accuracy: 0.8342\u001b[0m\n",
      "\u001b[34m7808/8000 [============================>.] - ETA: 0s - loss: 0.5397 - accuracy: 0.8348\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.5375 - accuracy: 0.8351 - val_loss: 0.3731 - val_accuracy: 0.9075\u001b[0m\n",
      "\u001b[34mEpoch 6/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.5103 - accuracy: 0.8672\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.4471 - accuracy: 0.8594\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.4748 - accuracy: 0.8484\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.4880 - accuracy: 0.8473\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.4851 - accuracy: 0.8473\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.4751 - accuracy: 0.8501\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.4789 - accuracy: 0.8487\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.4725 - accuracy: 0.8519\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.4671 - accuracy: 0.8557\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.4640 - accuracy: 0.8565 - val_loss: 0.3335 - val_accuracy: 0.9210\u001b[0m\n",
      "\u001b[34mEpoch 7/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4030 - accuracy: 0.8672\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8545\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.4629 - accuracy: 0.8562\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.4515 - accuracy: 0.8604\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.4418 - accuracy: 0.8622\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.4428 - accuracy: 0.8620\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.4307 - accuracy: 0.8674\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.4239 - accuracy: 0.8707\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.4224 - accuracy: 0.8714\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.4242 - accuracy: 0.8706 - val_loss: 0.3007 - val_accuracy: 0.9220\u001b[0m\n",
      "\u001b[34mEpoch 8/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4357 - accuracy: 0.8438\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.4561 - accuracy: 0.8613\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.4227 - accuracy: 0.8703\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.4255 - accuracy: 0.8706\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.4156 - accuracy: 0.8706\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.4084 - accuracy: 0.8727\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.4004 - accuracy: 0.8750\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3970 - accuracy: 0.8762\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.3942 - accuracy: 0.8789\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.3921 - accuracy: 0.8796 - val_loss: 0.3016 - val_accuracy: 0.9170\u001b[0m\n",
      "\u001b[34mEpoch 9/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.5088 - accuracy: 0.8516\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.3850 - accuracy: 0.8779\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.3783 - accuracy: 0.8839\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.3589 - accuracy: 0.8906\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.3643 - accuracy: 0.8891\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.3739 - accuracy: 0.8851\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.3707 - accuracy: 0.8849\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.3715 - accuracy: 0.8849\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.3698 - accuracy: 0.8862\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.3696 - accuracy: 0.8873 - val_loss: 0.2572 - val_accuracy: 0.9310\u001b[0m\n",
      "\u001b[34mEpoch 10/30\n",
      "\u001b[0m\n",
      "\u001b[34m 128/8000 [..............................] - ETA: 0s - loss: 0.3163 - accuracy: 0.9141\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3523 - accuracy: 0.8976\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 0s - loss: 0.3642 - accuracy: 0.8887\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.3561 - accuracy: 0.8899\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.3588 - accuracy: 0.8901\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.3542 - accuracy: 0.8908\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.3575 - accuracy: 0.8887\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3543 - accuracy: 0.8897\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.3516 - accuracy: 0.8917\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.3531 - accuracy: 0.8916 - val_loss: 0.2627 - val_accuracy: 0.9280\u001b[0m\n",
      "\u001b[34mEpoch 11/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8359\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.3686 - accuracy: 0.8906\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.3310 - accuracy: 0.8958\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.3308 - accuracy: 0.8974\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.3322 - accuracy: 0.8976\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.3277 - accuracy: 0.8995\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.3279 - accuracy: 0.8999\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.3284 - accuracy: 0.9002\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3315 - accuracy: 0.8982\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 62us/step - loss: 0.3308 - accuracy: 0.8976 - val_loss: 0.2488 - val_accuracy: 0.9365\u001b[0m\n",
      "\u001b[34mEpoch 12/30\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m 128/8000 [..............................] - ETA: 0s - loss: 0.3644 - accuracy: 0.8906\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2979 - accuracy: 0.9082\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.3065 - accuracy: 0.9057\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.3115 - accuracy: 0.9062\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 0s - loss: 0.3094 - accuracy: 0.9040\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.3064 - accuracy: 0.9050\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.3123 - accuracy: 0.9034\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.3127 - accuracy: 0.9033\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.9047\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.3129 - accuracy: 0.9044 - val_loss: 0.2641 - val_accuracy: 0.9270\u001b[0m\n",
      "\u001b[34mEpoch 13/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4917 - accuracy: 0.8438\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3269 - accuracy: 0.9028\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 0s - loss: 0.3059 - accuracy: 0.9126\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.3161 - accuracy: 0.9062\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.3144 - accuracy: 0.9060\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.3170 - accuracy: 0.9062\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.3119 - accuracy: 0.9062\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.3092 - accuracy: 0.9072\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.3072 - accuracy: 0.9073\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.3033 - accuracy: 0.9084 - val_loss: 0.2330 - val_accuracy: 0.9360\u001b[0m\n",
      "\u001b[34mEpoch 14/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2357 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.3015 - accuracy: 0.9123\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2759 - accuracy: 0.9196\u001b[0m\n",
      "\u001b[34m3072/8000 [==========>...................] - ETA: 0s - loss: 0.2775 - accuracy: 0.9186\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 0s - loss: 0.2729 - accuracy: 0.9206\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.2806 - accuracy: 0.9169\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2857 - accuracy: 0.9144\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2856 - accuracy: 0.9132\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.2887 - accuracy: 0.9118\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.2869 - accuracy: 0.9128 - val_loss: 0.2222 - val_accuracy: 0.9355\u001b[0m\n",
      "\u001b[34mEpoch 15/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1584 - accuracy: 0.9531\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2559 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.3008 - accuracy: 0.9068\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2829 - accuracy: 0.9124\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.2906 - accuracy: 0.9115\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2836 - accuracy: 0.9136\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.2771 - accuracy: 0.9151\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.2784 - accuracy: 0.9145\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.2761 - accuracy: 0.9158\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.2775 - accuracy: 0.9154 - val_loss: 0.2091 - val_accuracy: 0.9405\u001b[0m\n",
      "\u001b[34mEpoch 16/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2904 - accuracy: 0.8984\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2981 - accuracy: 0.9053\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2781 - accuracy: 0.9151\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.2924 - accuracy: 0.9119\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.2757 - accuracy: 0.9165\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.2677 - accuracy: 0.9188\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.2640 - accuracy: 0.9175\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.2624 - accuracy: 0.9181\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.2616 - accuracy: 0.9182\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.2619 - accuracy: 0.9180 - val_loss: 0.2046 - val_accuracy: 0.9395\u001b[0m\n",
      "\u001b[34mEpoch 17/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1823 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2075 - accuracy: 0.9375\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 0s - loss: 0.2348 - accuracy: 0.9341\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2415 - accuracy: 0.9290\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 0s - loss: 0.2553 - accuracy: 0.9254\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.2553 - accuracy: 0.9248\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2520 - accuracy: 0.9251\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2462 - accuracy: 0.9269\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2509 - accuracy: 0.9262\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 60us/step - loss: 0.2518 - accuracy: 0.9255 - val_loss: 0.1989 - val_accuracy: 0.9445\u001b[0m\n",
      "\u001b[34mEpoch 18/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2355 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2237 - accuracy: 0.9316\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2391 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.2275 - accuracy: 0.9315\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.2401 - accuracy: 0.9267\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.2421 - accuracy: 0.9258\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.2382 - accuracy: 0.9253\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.2443 - accuracy: 0.9250\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.2390 - accuracy: 0.9263\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.2414 - accuracy: 0.9262 - val_loss: 0.2306 - val_accuracy: 0.9335\u001b[0m\n",
      "\u001b[34mEpoch 19/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3600 - accuracy: 0.8984\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2780 - accuracy: 0.9141\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2459 - accuracy: 0.9234\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.2313 - accuracy: 0.9293\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.2358 - accuracy: 0.9270\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2372 - accuracy: 0.9248\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2421 - accuracy: 0.9245\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.2384 - accuracy: 0.9258\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2329 - accuracy: 0.9266\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 60us/step - loss: 0.2336 - accuracy: 0.9262 - val_loss: 0.2118 - val_accuracy: 0.9390\u001b[0m\n",
      "\u001b[34mEpoch 20/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1730 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1540 - accuracy: 0.9502\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2069 - accuracy: 0.9365\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2171 - accuracy: 0.9334\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.2228 - accuracy: 0.9331\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2243 - accuracy: 0.9320\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.2263 - accuracy: 0.9300\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.2227 - accuracy: 0.9309\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2254 - accuracy: 0.9306\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 1s 63us/step - loss: 0.2228 - accuracy: 0.9321 - val_loss: 0.1946 - val_accuracy: 0.9475\u001b[0m\n",
      "\u001b[34mEpoch 21/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1999 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.2012 - accuracy: 0.9375\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2138 - accuracy: 0.9347\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2148 - accuracy: 0.9347\u001b[0m\n",
      "\u001b[34m4224/8000 [==============>...............] - ETA: 0s - loss: 0.2124 - accuracy: 0.9339\u001b[0m\n",
      "\u001b[34m5120/8000 [==================>...........] - ETA: 0s - loss: 0.2159 - accuracy: 0.9314\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.2118 - accuracy: 0.9330\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.2104 - accuracy: 0.9333\u001b[0m\n",
      "\u001b[34m7936/8000 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9346\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 60us/step - loss: 0.2102 - accuracy: 0.9342 - val_loss: 0.1798 - val_accuracy: 0.9495\u001b[0m\n",
      "\u001b[34mEpoch 22/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1822 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1948 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1975 - accuracy: 0.9448\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.2003 - accuracy: 0.9409\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.2016 - accuracy: 0.9409\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.1988 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2020 - accuracy: 0.9395\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2060 - accuracy: 0.9390\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2056 - accuracy: 0.9385\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.2046 - accuracy: 0.9386 - val_loss: 0.1681 - val_accuracy: 0.9525\u001b[0m\n",
      "\u001b[34mEpoch 23/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1728 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.2076 - accuracy: 0.9385\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.2043 - accuracy: 0.9328\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1863 - accuracy: 0.9399\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 0s - loss: 0.1831 - accuracy: 0.9413\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.1863 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1928 - accuracy: 0.9400\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.1913 - accuracy: 0.9405\u001b[0m\n",
      "\u001b[34m7808/8000 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9402\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 60us/step - loss: 0.1929 - accuracy: 0.9402 - val_loss: 0.1809 - val_accuracy: 0.9495\u001b[0m\n",
      "\u001b[34mEpoch 24/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1451 - accuracy: 0.9609\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1959 - accuracy: 0.9366\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1943 - accuracy: 0.9375\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1923 - accuracy: 0.9412\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.1846 - accuracy: 0.9435\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1786 - accuracy: 0.9455\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.1794 - accuracy: 0.9462\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.1844 - accuracy: 0.9444\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.1839 - accuracy: 0.9436\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 62us/step - loss: 0.1870 - accuracy: 0.9424 - val_loss: 0.1683 - val_accuracy: 0.9505\u001b[0m\n",
      "\u001b[34mEpoch 25/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1554 - accuracy: 0.9688\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1656 - accuracy: 0.9492\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1782 - accuracy: 0.9443\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1794 - accuracy: 0.9432\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.1796 - accuracy: 0.9450\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.1908 - accuracy: 0.9410\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.1825 - accuracy: 0.9435\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1812 - accuracy: 0.9431\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1824 - accuracy: 0.9428\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.1809 - accuracy: 0.9424 - val_loss: 0.1718 - val_accuracy: 0.9505\u001b[0m\n",
      "\u001b[34mEpoch 26/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1571 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1759 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1731 - accuracy: 0.9435\u001b[0m\n",
      "\u001b[34m3072/8000 [==========>...................] - ETA: 0s - loss: 0.1749 - accuracy: 0.9417\u001b[0m\n",
      "\u001b[34m3968/8000 [=============>................] - ETA: 0s - loss: 0.1732 - accuracy: 0.9435\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.1731 - accuracy: 0.9441\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.1707 - accuracy: 0.9457\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9458\u001b[0m\n",
      "\u001b[34m7808/8000 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9472\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 60us/step - loss: 0.1729 - accuracy: 0.9479 - val_loss: 0.1851 - val_accuracy: 0.9450\u001b[0m\n",
      "\u001b[34mEpoch 27/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1941 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1619 - accuracy: 0.9473\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1542 - accuracy: 0.9521\u001b[0m\n",
      "\u001b[34m2816/8000 [=========>....................] - ETA: 0s - loss: 0.1571 - accuracy: 0.9496\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.1521 - accuracy: 0.9523\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.1592 - accuracy: 0.9488\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.1644 - accuracy: 0.9475\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1651 - accuracy: 0.9470\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1654 - accuracy: 0.9471\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 1s 63us/step - loss: 0.1646 - accuracy: 0.9476 - val_loss: 0.1552 - val_accuracy: 0.9535\u001b[0m\n",
      "\u001b[34mEpoch 28/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1344 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1024/8000 [==>...........................] - ETA: 0s - loss: 0.1538 - accuracy: 0.9541\u001b[0m\n",
      "\u001b[34m1920/8000 [======>.......................] - ETA: 0s - loss: 0.1647 - accuracy: 0.9521\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1632 - accuracy: 0.9507\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.1638 - accuracy: 0.9513\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1637 - accuracy: 0.9512\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1586 - accuracy: 0.9535\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.1618 - accuracy: 0.9517\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.1613 - accuracy: 0.9519\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.1592 - accuracy: 0.9520 - val_loss: 0.1617 - val_accuracy: 0.9535\u001b[0m\n",
      "\u001b[34mEpoch 29/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1090 - accuracy: 0.9766\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1333 - accuracy: 0.9609\u001b[0m\n",
      "\u001b[34m2048/8000 [======>.......................] - ETA: 0s - loss: 0.1404 - accuracy: 0.9590\u001b[0m\n",
      "\u001b[34m2944/8000 [==========>...................] - ETA: 0s - loss: 0.1434 - accuracy: 0.9589\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.1475 - accuracy: 0.9570\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1440 - accuracy: 0.9584\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1471 - accuracy: 0.9572\u001b[0m\n",
      "\u001b[34m6656/8000 [=======================>......] - ETA: 0s - loss: 0.1513 - accuracy: 0.9554\u001b[0m\n",
      "\u001b[34m7552/8000 [===========================>..] - ETA: 0s - loss: 0.1506 - accuracy: 0.9551\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 61us/step - loss: 0.1516 - accuracy: 0.9546 - val_loss: 0.1556 - val_accuracy: 0.9580\u001b[0m\n",
      "\u001b[34mEpoch 30/30\n",
      "\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1208 - accuracy: 0.9609\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1455 - accuracy: 0.9601\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1242 - accuracy: 0.9660\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1293 - accuracy: 0.9650\u001b[0m\n",
      "\u001b[34m4096/8000 [==============>...............] - ETA: 0s - loss: 0.1340 - accuracy: 0.9612\u001b[0m\n",
      "\u001b[34m5120/8000 [==================>...........] - ETA: 0s - loss: 0.1411 - accuracy: 0.9588\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.1401 - accuracy: 0.9596\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.1433 - accuracy: 0.9580\u001b[0m\n",
      "\u001b[34m7808/8000 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.9581\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 62us/step - loss: 0.1421 - accuracy: 0.9584 - val_loss: 0.1634 - val_accuracy: 0.9555\n",
      "\u001b[0m\n",
      "\u001b[34mFinished training the model.\u001b[0m\n",
      "\u001b[34mTest loss: 0.16335211190208793\u001b[0m\n",
      "\u001b[34mTest accuracy: 0.9555000066757202\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mFinished saving the model.\u001b[0m\n",
      "\u001b[34mFinished training the model.\n",
      "\u001b[0m\n",
      "\u001b[34mScript Status - Finished\n",
      "\u001b[0m\n",
      "\u001b[34mTotal time taken:  22.887819290161133\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-04-17 07:42:53 Uploading - Uploading generated training model\n",
      "2020-04-17 07:42:53 Completed - Training job completed\n",
      "Training seconds: 108\n",
      "Billable seconds: 108\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(inputs=data_location, \n",
    "              logs=True, \n",
    "              job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! We had a successful training job run in Amazon SageMaker.\n",
    "#### Please return to the tutorial for Part 6 where we will be running a training job in a GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-keras-mlflow-sagemaker",
   "language": "python",
   "name": "tensorflow-keras-mlflow-sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
